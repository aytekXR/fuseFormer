\begin{thebibliography}{10}

\bibitem{li2021rfn}
H.~Li, X.-J. Wu, and J.~Kittler, ``Rfn-nest: An end-to-end residual fusion
  network for infrared and visible images,'' {\em Information Fusion}, vol.~73,
  pp.~72--86, 2021.

\bibitem{bin2016efficient}
Y.~Bin, Y.~Chao, and H.~Guoyu, ``Efficient image fusion with approximate sparse
  representation,'' {\em International Journal of Wavelets, Multiresolution and
  Information Processing}, vol.~14, no.~04, p.~1650024, 2016.

\bibitem{zhang2013dictionary}
Q.~Zhang, Y.~Fu, H.~Li, and J.~Zou, ``Dictionary learning method for joint
  sparse representation-based image fusion,'' {\em Optical Engineering},
  vol.~52, no.~5, pp.~057006--057006, 2013.

\bibitem{hu2017adaptive}
H.-M. Hu, J.~Wu, B.~Li, Q.~Guo, and J.~Zheng, ``An adaptive fusion algorithm
  for visible and infrared videos based on entropy and the cumulative
  distribution of gray levels,'' {\em IEEE Transactions on Multimedia},
  vol.~19, no.~12, pp.~2706--2719, 2017.

\bibitem{he2017infrared}
K.~He, D.~Zhou, X.~Zhang, R.~Nie, Q.~Wang, and X.~Jin, ``Infrared and visible
  image fusion based on target extraction in the nonsubsampled contourlet
  transform domain,'' {\em Journal of Applied Remote Sensing}, vol.~11, no.~1,
  pp.~015011--015011, 2017.

\bibitem{liu2012robust}
G.~Liu, Z.~Lin, S.~Yan, J.~Sun, Y.~Yu, and Y.~Ma, ``Robust recovery of subspace
  structures by low-rank representation,'' {\em IEEE transactions on pattern
  analysis and machine intelligence}, vol.~35, no.~1, pp.~171--184, 2012.

\bibitem{liu2018infrared}
Y.~Liu, X.~Chen, J.~Cheng, H.~Peng, and Z.~Wang, ``Infrared and visible image
  fusion with convolutional neural networks,'' {\em International Journal of
  Wavelets, Multiresolution and Information Processing}, vol.~16, no.~03,
  p.~1850018, 2018.

\bibitem{li2019infrared}
H.~Li, X.-j. Wu, and T.~S. Durrani, ``Infrared and visible image fusion with
  resnet and zero-phase component analysis,'' {\em Infrared Physics \&
  Technology}, vol.~102, p.~103039, 2019.

\bibitem{raza2020pfaf}
A.~Raza, H.~Huo, and T.~Fang, ``Pfaf-net: Pyramid feature network for
  multimodal fusion,'' {\em IEEE Sensors Letters}, vol.~4, no.~12, pp.~1--4,
  2020.

\bibitem{fu2021dual}
Y.~Fu and X.-J. Wu, ``A dual-branch network for infrared and visible image
  fusion,'' in {\em 2020 25th International Conference on Pattern Recognition
  (ICPR)}, pp.~10675--10680, IEEE, 2021.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' in {\em
  Advances in neural information processing systems}, pp.~2672--2680, 2014.

\bibitem{ma2020ganmcc}
J.~Ma, H.~Zhang, Z.~Shao, P.~Liang, and H.~Xu, ``Ganmcc: A generative
  adversarial network with multiclassification constraints for infrared and
  visible image fusion,'' {\em IEEE Transactions on Instrumentation and
  Measurement}, vol.~70, pp.~1--14, 2020.

\bibitem{xu2019learning}
H.~Xu, P.~Liang, W.~Yu, J.~Jiang, and J.~Ma, ``Learning a generative model for
  fusing infrared and visible images via conditional generative adversarial
  network with dual discriminators.,'' in {\em IJCAI}, pp.~3954--3960, 2019.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, {\em et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in {\em
  Proceedings of the IEEE/CVF international conference on computer vision},
  pp.~10012--10022, 2021.

\bibitem{liu2022mfst}
X.~Liu, H.~Gao, Q.~Miao, Y.~Xi, Y.~Ai, and D.~Gao, ``Mfst: Multi-modal feature
  self-adaptive transformer for infrared and visible image fusion,'' {\em
  Remote Sensing}, vol.~14, no.~13, p.~3233, 2022.

\bibitem{zhao2021dndt}
H.~Zhao and R.~Nie, ``Dndt: Infrared and visible image fusion via densenet and
  dual-transformer,'' in {\em 2021 International Conference on Information
  Technology and Biomedical Engineering (ICITBE)}, pp.~71--75, IEEE, 2021.

\bibitem{rao2023tgfuse}
D.~Rao, T.~Xu, and X.-J. Wu, ``Tgfuse: An infrared and visible image fusion
  approach based on transformer and generative adversarial network,'' {\em IEEE
  Transactions on Image Processing}, 2023.

\bibitem{li2022cgtf}
J.~Li, J.~Zhu, C.~Li, X.~Chen, and B.~Yang, ``Cgtf: Convolution-guided
  transformer for infrared and visible image fusion,'' {\em IEEE Transactions
  on Instrumentation and Measurement}, vol.~71, pp.~1--14, 2022.

\bibitem{tang2022ydtr}
W.~Tang, F.~He, and Y.~Liu, ``Ydtr: infrared and visible image fusion via
  y-shape dynamic transformer,'' {\em IEEE Transactions on Multimedia}, 2022.

\bibitem{wang2022swinfuse}
Z.~Wang, Y.~Chen, W.~Shao, H.~Li, and L.~Zhang, ``Swinfuse: A residual swin
  transformer fusion network for infrared and visible images,'' {\em IEEE
  Transactions on Instrumentation and Measurement}, vol.~71, pp.~1--12, 2022.

\bibitem{yang2023dglt}
X.~Yang, H.~Huo, R.~Wang, C.~Li, X.~Liu, and J.~Li, ``Dglt-fusion: A decoupled
  global--local infrared and visible image fusion transformer,'' {\em Infrared
  Physics \& Technology}, vol.~128, p.~104522, 2023.

\bibitem{tang2023tccfusion}
W.~Tang, F.~He, and Y.~Liu, ``Tccfusion: An infrared and visible image fusion
  method based on transformer and cross correlation,'' {\em Pattern
  Recognition}, p.~109295, 2023.

\bibitem{vs2022image}
V.~Vs, J.~M.~J. Valanarasu, P.~Oza, and V.~M. Patel, ``Image fusion
  transformer,'' in {\em 2022 IEEE International Conference on Image Processing
  (ICIP)}, pp.~3566--3570, IEEE, 2022.

\bibitem{fu2021ppt}
Y.~Fu, T.~Xu, X.~Wu, and J.~Kittler, ``Ppt fusion: Pyramid patch transformerfor
  a case study in image fusion,'' {\em arXiv preprint arXiv:2107.13967}, 2021.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' in {\em Computer Vision--ECCV 2014: 13th European Conference,
  Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pp.~740--755, Springer, 2014.

\bibitem{xu2020aaai}
H.~Xu, J.~Ma, Z.~Le, J.~Jiang, and X.~Guo, ``Fusiondn: A unified densely
  connected network for image fusion,'' in {\em proceedings of the
  Thirty-Fourth AAAI Conference on Artificial Intelligence}, 2020.

\bibitem{toet2014tno}
A.~Toet {\em et~al.}, ``Tno image fusion dataset< https://figshare.
  com/articles,'' {\em TN\_Image\_Fusion\_Dataset/1008029}, 2014.

\bibitem{ma2015perceptual}
K.~Ma, K.~Zeng, and Z.~Wang, ``Perceptual quality assessment for multi-exposure
  image fusion,'' {\em IEEE Transactions on Image Processing}, vol.~24, no.~11,
  pp.~3345--3356, 2015.

\bibitem{ho2019axial}
J.~Ho, N.~Kalchbrenner, D.~Weissenborn, and T.~Salimans, ``Axial attention in
  multidimensional transformers,'' {\em arXiv preprint arXiv:1912.12180}, 2019.

\bibitem{wang2020axial}
H.~Wang, Y.~Zhu, B.~Green, H.~Adam, A.~Yuille, and L.-C. Chen, ``Axial-deeplab:
  Stand-alone axial-attention for panoptic segmentation,'' in {\em European
  conference on computer vision}, pp.~108--126, Springer, 2020.

\end{thebibliography}
