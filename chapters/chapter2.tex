\label{chp:RelatedWork}

The domain of RGB and infrared image fusion has been a focal point of substantial research, witnessing a plethora of innovative methods from the early 1990s to the cutting-edge Transformer-based models. These techniques span categories like Traditional Fusion Algorithms, CNN, Autoencoder, GAN, and notably, Transformer-based approaches where attention mechanisms optimize the fusion. Within this vast spectrum, a key challenge is the limited availability of labeled datasets due to the inherent complexity of fusing RGB and IR images. As a result, our research navigates the unsupervised scenario, leveraging various Performance Evaluation and Benchmarking metrics to quantify the fusion results, offering insights into the strengths, limitations, and practical applications of each method. This chapter culminates in our exploration of Transformer-based algorithms, representing a comprehensive journey through RGB and IR image fusion techniques and challenges.

Image fusion algorithms can be classified based on factors like the use of learning methods versus hand-crafted steps, predefined loss functions, and the involvement of labeled datasets. While learning-based methods like CNN, GAN, Transformers, and Auto-encoders harness machine learning to understand input images, hand-crafted approaches involve manual feature design and fusion rules. The categorization extends to whether methods are end-to-end or require intermediary handcrafted steps. Furthermore, loss functions guide the training process, leading to classifications like self-supervised, supervised, or unsupervised based on the loss function. Labeled datasets, whether for training learning-based methods where ground-truth is known or for evaluating fusion algorithms, play a pivotal role. These considerations help researchers select the most apt image fusion algorithm tailored to their application requirements.

\subsubsection{Traditional Fusion Algorithms}
\label{sec:traditional}

Traditional image fusion algorithms, while extensively studied, come with inherent shortcomings, notably the presence of handcrafted steps leading to potentially suboptimal results and high time complexity in some cases. Sparse representation (SR) based methods, such as those by Bin et al \cite{bin2016efficient} and Zhang et al \cite{zhang2013dictionary}, are popular but are limited by requirements like dictionary learning, which increases their time complexity, and handcrafted steps affecting their generalizability. Multi-scale transformation (MST) based methods, exemplified by Hu et al \cite{hu2017adaptive} and others \cite{he2017infrared}, are effective at capturing image characteristics at varying scales but can lack generalizability in specific contexts. Low-rank representation (LRR) methods, like those from Liu et al \cite{liu2012robust}, excel in handling noise and image degradation, but may falter with complex textures or patterns. Ultimately, the efficacy of traditional fusion algorithms largely hinges on the chosen feature extraction method, necessitating careful consideration of the problem's specific demands before settling on a technique.

\subsection{Learning Based Algorithms for Image Fusion}

\subsubsection{CNN Based Algorithms}
Liu et al. were among the first to introduce a method for image fusion that utilizes CNN, focusing on the fusion of infrared and visible images \cite{liu2018infrared}. Their approach consisted of several steps, from preprocessing to feature extraction, fusion strategy, and reconstruction, achieving better fusion results than other state-of-the-art methods of the time. CNN-based methods are generally divided into supervised and unsupervised categories. While both utilize CNNs for feature extraction, supervised methods require labeled data, often leveraging techniques like data augmentation or transfer learning. Unsupservised methods, on the other hand, don't require labeled data, making them more flexible for real-world applications. Despite their success, there's room for improvement, especially in situations where input images have significant differences in factors like illumination or resolution.

\subsubsection{Autoencoder Based Algorithms}
Autoencoders, a type of neural network, have been widely used for tasks like dimensionality reduction and data compression. Within the domain of infrared visual image fusion, autoencoders are leveraged to extract features from source images in the encoder stage, while the decoder stage focuses on reconstructing the fused image. The training process involves two stages: firstly, training the autoencoder using source images without any fusion, followed by integrating the fusion step. Notable works in this area include DenseFuse \cite{li2019infrared} and studies by Raza et al. \cite{raza2020pfaf} and Fu et al. \cite{fu2021dual}, among others.

\subsubsection{GAN Based Algorithms}
Since their introduction by Goodfellow et al. \cite{goodfellow2014generative}, GANs have found diverse applications, including image fusion. The majority of GAN-based image fusion methods are unsupervised, focusing on the difference between the fused and source images. Innovations in this space include the use of multiple discriminators, as seen in methods introduced by Ma et al. \cite{ma2020ganmcc} and Xu et al. \cite{xu2019learning}, and the integration of attention mechanisms and residual connections for improved performance.

\subsubsection{Transformer Based Algorithms}
\label{sec:Transformer}

Transformers have risen as paramount tools in managing long-range dependencies in fields like natural language processing and computer vision \cite{dosovitskiy2020image, liu2021swin, liu2022mfst}. Their introduction into image fusion in 2021 revolutionized the domain, with several innovative methodologies emerging \cite{zhao2021dndt, rao2023tgfuse, li2022cgtf, tang2022ydtr, wang2022swinfuse, yang2023dglt, tang2023tccfusion}. 

Central to these methodologies is the self-attention mechanism, enhancing fusion outcomes by preserving essential details and efficiently blending features. For instance, VS et al. \cite{vs2022image} introduced a multiscale fusion strategy harnessing this architecture, while Zhao et al. \cite{zhao2021dndt} advanced a dual transformer approach. Noteworthy strategies also include the integration of transformers with traditional methods, as evidenced by Fu et al.\cite{fu2021ppt}, Wang et al. \cite{wang2022swinfuse}, and others.

Transformer-based VIF techniques predominantly operate unsupervised, leveraging loss functions from the comparison of fused and source images. While this eliminates the need for annotated data, it also complicates methodological evaluations due to the absence of a direct quality reference.

With increasing research into transformer-based image fusion, the focus remains on novel designs and methodologies, such as diverse transformer integrations in the fusion pipeline, transformer-CNN combinations, and the utilization of auxiliary information to enrich fusion processes.
