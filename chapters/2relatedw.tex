\label{chp:RelatedWork}
\label{sec:traditional}
\label{sec:Transformer}
The RGB and infrared image fusion domain has undergone extensive research, traversing from Traditional Fusion Algorithms to cutting-edge Transformer-based models \cite{bin2016efficient, zhang2013dictionary, hu2017adaptive, he2017infrared, liu2012robust}. In the early 1990s, methods like Sparse Representation and Multi-scale Transformation were explored, each with its inherent limitations. Traditional algorithms, relying on handcrafted steps, faced challenges in adaptability and time complexity \cite{bin2016efficient, zhang2013dictionary, hu2017adaptive, he2017infrared, liu2012robust}. The scarcity of labeled datasets for RGB-IR fusion prompted a shift towards unsupervised scenarios, guiding our exploration of Performance Evaluation metrics.

With the advent of deep learning, learning-based algorithms became predominant, categorized by learning methods, loss functions, and the use of labeled datasets \cite{liu2018infrared, li2019infrared, raza2020pfaf, fu2021dual, goodfellow2014generative}. CNN-based approaches, both supervised and unsupervised, exhibited success in feature extraction for image fusion, yet challenges persisted in scenarios with significant differences in factors like illumination or resolution \cite{liu2018infrared}. Autoencoder-based algorithms, utilizing neural networks for dimensionality reduction, showcased advancements in works such as DenseFuse, Raza et al., and Fu et al. \cite{li2019infrared, raza2020pfaf, fu2021dual}.

GAN-based methods, introduced by Goodfellow et al., focused on unsupervised fusion, integrating attention mechanisms and residual connections for improved performance \cite{goodfellow2014generative, ma2020ganmcc, xu2019learning}. While these approaches demonstrated promise, challenges persisted in effectively handling the inherent differences between fused and source images.

A transformative shift in RGB-IR image fusion occurred with the introduction of Transformer-based algorithms in 2021 \cite{dosovitskiy2020image, liu2021swin, liu2022mfst}. These methodologies, driven by the self-attention mechanism, marked a paradigm shift by efficiently managing long-range dependencies in images. Innovative designs, such as multiscale fusion strategies and dual transformer approaches, were introduced, emphasizing the seamless integration of Transformers with traditional methods \cite{vs2022image, zhao2021dndt, fu2021ppt, wang2022swinfuse}. Unsupervised Transformer-based techniques, reliant on loss functions, eliminated the need for labeled data but posed challenges in methodological evaluations \cite{vs2022image, zhao2021dndt}. Ongoing research explores diverse Transformer integrations, Transformer-CNN combinations, and the utilization of auxiliary information to further enrich the fusion process \cite{vs2022image, zhao2021dndt, fu2021ppt, wang2022swinfuse}.

This section provides a comprehensive overview of the evolution of RGB-IR image fusion techniques, emphasizing the pivotal role of Transformer-based approaches as the forefront of current research in the field \cite{vs2022image, zhao2021dndt, fu2021ppt, wang2022swinfuse}. The integration of deep learning methods has not only enhanced feature extraction capabilities but has also paved the way for more adaptive and robust solutions in the challenging realm of image fusion.