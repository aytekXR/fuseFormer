\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{FuseFormer: A Transformer For Visual and Infrared Image Fusion\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Aytekin Erdogan}
\IEEEauthorblockA{\textit{Graduate School of Informatics} \\
\textit{Middle East Technical University}\\
Ankara, Turkey \\
aytekin.erdogan@metu.edu.tr}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Erdem Akagunduz}
\IEEEauthorblockA{\textit{Graduate School of Informatics} \\
\textit{Middle East Technical University}\\
Ankara, Turkey \\
akaerdem@metu.edu.tr}
}

\maketitle

\begin{abstract}
Image fusion is a process where images obtained from different sensors are combined to generate a single image that benefits from complementary information. Recently, there has been a growing interest in image fusion, which involves fusing images from diverse sensors to produce an enhanced image. Although deep learning methods have been widely employed in state-of-the-art techniques to extract meaningful features for image fusion, these methods primarily focus on integrating local features while disregarding the broader context within the image. To overcome this limitation, Transformer-based models have emerged as a promising solution, aiming to capture general context dependencies through attention mechanisms. Inspired by this, we propose a novel image fusion approach that incorporates a transformer-based multi-scale fusion strategy, effectively considering both local and general context information, thus enhancing the overall fusion process. Our proposed method follows a two-stage training approach, where an auto-encoder is initially trained to extract deep features at multiple scales. Subsequently, the multi-scale features are fused using a combination of Convolutional Neural Networks (CNNs) and Transformers. The CNNs are utilized to capture local features, while the Transformer handles the integration of general context features. Notably, in contrast to similar methods, we propose novel loss functions to address the challenges associated with defining a loss function when ground truth for fusion is absent. Through extensive experiments on various benchmark datasets, our proposed method, along with the novel loss function definition, demonstrates superior performance compared to other competitive fusion algorithms. Overall, this thesis presents significant advancements in image fusion techniques, offering innovative approaches and contributing to the state-of-the-art in this field.
\end{abstract}

\begin{IEEEkeywords}
Image Fusion, Visual Infrared Image Fusion, Transformer Based Image Fusion, Structural Similarity Metric
\end{IEEEkeywords}


\section{Introduction}
\input{chapters/chapter1}

\section{Related Work}
\input{chapters/chapter2}

\section{Methodology}
\input{chapters/chapter3}

\bibliographystyle{ieeetr} 
\bibliography{thesis}

\end{document}
